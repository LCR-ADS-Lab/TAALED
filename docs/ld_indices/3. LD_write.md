
# How to Save your Calculation

**LD write function** allows you to save your analysis into a *txt* file.

## 1. Import necessary packages
```python
from taaled import ld
from pylats import lats

#for creating an output filename
from datetime import datetime 
from datetime import date

#for finding a list of texts
import glob
```


## 2. Get a list of corpus files

For the analysis, we are going to use the sample essays from [the Gachon Learner Corpus](https://app.box.com/s/vw4803lct2dq4xbrquae).

```python
#set a path to the folder where your corpus located in
corp_path = "/Users/Unknown/Documents/TAALED/Corpus/sample" 

#get a list of filenames from the folder (need to tweak depending on your filenames)
files = glob.glob(corp_path + "/*.txt")

#(optional) check the number of the files in your folder
len(files) #16111
```

## 3. Define a file-naming function

```python
def outname_creator(fldname,isprll,other = None):
	day = date.today().strftime("%Y%m%d") #get date
	time = datetime.now().strftime("%H%M%S")
	ldv = "taaledv" + ld.version
	latsv = "pylatsv" + lats.version
	if isprll == True:
		pa = "pa"
	else:
		pa = "nopa"

	if other == None:
		outn = "_".join([day,time,fldname,pa]) + ".txt"
	else:
		outn = "_".join([day,time,fldname,pa,other]) + ".txt"

	return(outn)
	
	
outname = outname_creator("sample",False)	
```

## 4. Make an output file 

We can calculate the indices with/without a parallel analysis

```python
#calculate ld for entire corpus (wihtout a parallel analysis)
#an output filename should be "date_time_sample_nopa(no parallel analysis).txt"
ld.ldwrite(sample,outname)

#run a parallel analysis for entire corpus
#an output filename should be "date_time_sample_pa(parallel analysis).txt"
ld.ldwrite(sample,outname_creator("sample",True),mx = 200, prll = True) #you can change the mx number
```

## 5. Examples of the output files

>When the corpus files successfully processed by TAALED, we will find an output file on the working directory.
<img width="300" alt="image" src="https://user-images.githubusercontent.com/84297888/160349463-445435e7-857a-4468-b463-19bfa471d588.png">

>The name of the file is "20220328_002547_sample_nopa.txt" (non-parallel).
<img width="600" alt="image" src="https://user-images.githubusercontent.com/84297888/160350060-bca6aa3d-4c25-4c1e-9348-12c1c79a71c4.png">

>If you want to do further (statistical) analysis with the calculated values, we can copy the output to other program (e.g., Excel).
<img width="600" alt="image" src="https://user-images.githubusercontent.com/84297888/160350579-0fb31906-db6f-4198-8d6e-34b4284575b3.png">

>While doing the parallel analysis, a text could be skipped if the text length is shorter than the mx number.
(For more information about the parallel analysis, read [LEARN MORE](#learn-more-parallel-analysis) below.

<img width="300" alt="image" src="https://user-images.githubusercontent.com/84297888/160351078-ffc5e397-b6c6-4fab-85c6-c7eb8f62b865.png">

>The name of the file is "20220328_004640_sample_pa.txt" (parallel).
<img width="600" alt="image" src="https://user-images.githubusercontent.com/84297888/160352696-212ddfb9-9d7f-434c-b3f8-66ab840e65e5.png">

<img width="600" alt="image" src="https://user-images.githubusercontent.com/84297888/160352807-acd748c3-9272-4556-9ff6-883728d0c1c0.png">


### ►LEARN MORE: Parallel Analysis 
- Hess et al.(1986) used the parallel sampling method to divide each language sample into texts of different lengths. First, each sample was clipped to the first 200 tokens. Then the resulting texts were subdivided into four texts of 50 tokens, two texts of 100 tokens, one text of 150 tokens, and one text of 200 tokens. LD scores were then calculated for each text, and values from texts of the same length were averaged. Subsequent analysis using repeated measures ANOVAs showed that all the LD measures were significantly affected by text length (e.g., η2 = .82 for simple TTR). Hess et al. concluded that these LD measures were not suitable for making comparisons between texts of different lengths
- First, each essay was clipped to the first 200 tokens. Then the parallel sampling method (Hess et al., 1986) was used to subdivide the essays into texts ranging from 50 to 200 tokens in length and increasing at increments of five tokens (four texts of 50 tokens, three texts of 55 tokens, etc.). Fifty tokens was chosen as the shortest text length because our procedure for calculating moving averages required a window of 50 tokens (Zenker & Kyle, 2021).
