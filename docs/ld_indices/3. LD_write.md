
# How to Save your Calculation

**LD write function** allows you to save your analysis into a *txt* file.
The following steps descript how to use this function.

## 1. Import necessary packages
```python
from taaled import ld
from pylats import lats

#for creating an output filename
from datetime import datetime 
from datetime import date

#for finding a list of texts
import glob
```


## 2. Get a list of corpus files

For the analysis, we are going to use the sample essays from [the Gachon Learner Corpus](http://koreanlearnercorpusblog.blogspot.com/p/corpus.html) (Carlstrom & Price, 2014). 
Here, we are going to use The Gachon Learner Corpus 2.1 (Text files).  

```python
#set a path to the folder where your corpus located in
corp_path = "/Users/Unknown/Documents/TAALED/Corpus/sample" 

#get a list of filenames from the folder (need to tweak depending on your filenames)
files = glob.glob(corp_path + "/*.txt")

#(optional) check the number of the files in your folder
len(files) #16111
```

## 3. Define a file-naming function
The following is a helper function to define the filename for your output. This is not absolutely necessary, but our experiences suggests that having timestamps on the output file will help enhance the transparency and replicability of the analysis (because we might change a small details which might make the output different at different points in time.).

The default format of the output filename is `{DATE}_{TIME}_{NAME YOU SPECIFY}_{whether to use parallel analysis or not}_{OTHER}.txt`.


```python
def outname_creator(fldname,isprll,other = None):
	day = date.today().strftime("%Y%m%d") #get date
	time = datetime.now().strftime("%H%M%S")
	ldv = "taaledv" + ld.version
	latsv = "pylatsv" + lats.version
	if isprll == True: # whether the output is based on parallel analysis or not 
		pa = "pa"
	else:
		pa = "nopa"

	if other == None:
		outn = "_".join([day,time,fldname,pa]) + ".txt"
	else:
		outn = "_".join([day,time,fldname,pa,other]) + ".txt"

	return(outn)
	
	
outname = outname_creator("sample",False)	
```

## 4. Make an output file

We can calculate the indices with/without a parallel analysis.

You can run the `ld.ldwrite` function without parallel analysis.
```python
#calculate ld for entire corpus (wihtout a parallel analysis)
#an output filename should be "date_time_sample_nopa(no parallel analysis).txt"
ld.ldwrite(files,outname)
```

You can set the `prll` argument `True` to implement parallel analysis in your analysis.
```python

#run a parallel analysis for entire corpus
#an output filename should be "date_time_sample_pa(parallel analysis).txt"
ld.ldwrite(files,outname_creator("sample",True),mx = 200, prll = True) #mx defines the maximum length of each parallel sample
```

## 5. Examples of the output files

>When the corpus files successfully processed by TAALED, we will find an output file on the working directory.
<img width="300" alt="image" src="https://user-images.githubusercontent.com/84297888/160349463-445435e7-857a-4468-b463-19bfa471d588.png">

>The name of the file is "20220328_002547_sample_nopa.txt" (non-parallel).
<img width="600" alt="image" src="https://user-images.githubusercontent.com/84297888/160350060-bca6aa3d-4c25-4c1e-9348-12c1c79a71c4.png">

>If you want to do further (statistical) analysis with the calculated values, we can copy the output to other program (e.g., Excel). Alternatively, some statistical programs can directly import this tab delimited files.
<img width="600" alt="image" src="https://user-images.githubusercontent.com/84297888/160350579-0fb31906-db6f-4198-8d6e-34b4284575b3.png">

>While doing [the parallel analysis](#learn-more-parallel-analysis), a text could be skipped if the text length is shorter than *the mx number*.
<img width="300" alt="image" src="https://user-images.githubusercontent.com/84297888/160351078-ffc5e397-b6c6-4fab-85c6-c7eb8f62b865.png">

>The name of the file is "20220328_004640_sample_pa.txt" (parallel).
<img width="600" alt="image" src="https://user-images.githubusercontent.com/84297888/160352696-212ddfb9-9d7f-434c-b3f8-66ab840e65e5.png">
<img width="600" alt="image" src="https://user-images.githubusercontent.com/84297888/160352807-acd748c3-9272-4556-9ff6-883728d0c1c0.png">


## LEARN MORE: Parallel Analysis

* Hess et al.(1986)<sup>[7](https://lcr-ads-lab.github.io/TAALED/references/1.%20Related%20Studies.html#hess-c-w-sefton-k-m--landry-r-g-1986)</sup> first used the parallel sampling method to objectively measure the indices from short text samples of young children of 3, 4, and 5 years of age. First, each sample was clipped to the first 200 tokens. Then the resulting texts were subdivided into four texts of 50 tokens, two texts of 100 tokens, one text of 150 tokens, and one text of 200 tokens. LD scores were then calculated for each text, and values from texts of the same length were averaged. Subsequent analysis using repeated measures ANOVAs showed that all the LD measures in the expreiment were significantly affected by text length. 

* The parallel analysis has been actively used in the previous studies related to LD indices. McCarthy and Jarvis (2007)<sup>[16](https://lcr-ads-lab.github.io/TAALED/references/1.%20Related%20Studies.html#mccarthy-p-m--jarvis-s-2007)</sup> selected nine representative samples from each genre and kept the consistent maximum text lengths up to 2000 tokens. The samples were divided into eleven different sections from 100 to 2000 tokens to use the parallel sampling method. Additionally, McCarthy and Jarvis (2010)<sup>[17](https://lcr-ads-lab.github.io/TAALED/references/1.%20Related%20Studies.html#mccarthy-p-m--jarvis-s-2010)</sup> tested MTLD and HD-D using the same method.

* Koizuimi (2012)<sup>[9](https://lcr-ads-lab.github.io/TAALED/references/1.%20Related%20Studies.html#koizumi-r-2012)</sup> and Koizumi and Inâ€™nami (2012)<sup>[10](https://lcr-ads-lab.github.io/TAALED/references/1.%20Related%20Studies.html#koizumi-r--innami-y-2012)</sup> clipped each spoken text to the first 200 tokens, and overall texts were divided into 25 segments.

* Zenker & Kyle (2021)<sup>[20](https://lcr-ads-lab.github.io/TAALED/references/1.%20Related%20Studies.html#zenker-f--kyle-k-2021)</sup> investigated the minimum text lengths needed to produce stable LD values in L2 written texts by clipping each text to the first 200 tokens and subdividing the essays into texts ranging from 50 to 200 tokens in length and increasing at increments of five tokens (four texts of 50 tokens, three texts of 55 tokens, etc.).


## Citation of the learner corpus we used on this page

Carlstrom, B. and Price, N. (2012-2014) The Gachon Learner Corpus. Available online at http://koreanlearnercorpusblog.blogspot.kr/p/corpus.html.
