
# Calculate revised LD indices
ðŸ’¡ For more information about the lexical diversity indices calculated by taaled, read **â–¶ï¸ŽLEARN MORE**.

**Create ldvals object**
```python
#create ld object
ldvals = ld.lexdiv(clnsmpl.toks)
```

## MATTR<sup>â˜…â˜…â˜…</sup>
The Moving-Average Type-Token Ratio calculates the moving average for all segments of a given length. For a segment length of 50 tokens, TTR is calculated on tokens 1-50, 2-51, 3-53, etc., and the resulting TTR measurements are averaged to produce the final MATTR value (Covington & McFall, 2010)<sup>[15](https://lcr-ads-lab.github.io/TAALED/references/1.%20Studies.html#covington-m-a--mcfall-j-d-2010)</sup>.

**Report mattr value**

```python
print(ldvals.mattr) #moving average TTR value
```
```
0.7922466960352423
```

**Check TTR values for each window**

```
print(ldvals.mattrs)
```

```
[0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.78, 0.78, 0.78, 0.78, 0.8, 0.82, 0.84, 0.82, 0.8, 0.8, 0.82, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.78, 0.8, 0.78, 0.76, 0.76, 0.76, 0.78, 0.78, 0.78, 0.78, 0.8, 0.8, 0.78, 0.76, 0.78, 0.78, 0.8, 0.82, 0.8, 0.82, 0.82, 0.82, 0.82, 0.8, 0.82, 0.82, 0.8, 0.78, 0.78, 0.8, 0.8, 0.8, 0.8, 0.8, 0.78, 0.78, 0.8, 0.82, 0.8, 0.8, 0.78, 0.8, 0.8, 0.8, 0.8, 0.8, 0.82, 0.8, 0.82, 0.82, 0.84, 0.84, 0.86, 0.86, 0.84, 0.84, 0.86, 0.84, 0.84, 0.84, 0.84, 0.84, 0.86, 0.84, 0.84, 0.82, 0.8, 0.78, 0.78, 0.8, 0.82, 0.82, 0.82, 0.82, 0.84, 0.84, 0.86, 0.86, 0.84, 0.84, 0.84, 0.84, 0.82, 0.82, 0.8, 0.78, 0.76, 0.76, 0.76, 0.76, 0.78, 0.8, 0.8, 0.78, 0.76, 0.76, 0.76, 0.78, 0.78, 0.78, 0.78, 0.76, 0.76, 0.74, 0.76, 0.76, 0.76, 0.78, 0.78, 0.78, 0.78, 0.78, 0.76, 0.78, 0.78, 0.8, 0.8, 0.8, 0.78, 0.78, 0.76, 0.76, 0.76, 0.78, 0.78, 0.78, 0.76, 0.76, 0.78, 0.76, 0.78, 0.76, 0.76, 0.76, 0.76, 0.76, 0.78, 0.78, 0.78, 0.78, 0.78, 0.78, 0.76, 0.78, 0.8, 0.8, 0.8, 0.8, 0.8, 0.82, 0.8, 0.8, 0.8, 0.78, 0.78, 0.8, 0.8, 0.82, 0.84, 0.82, 0.82, 0.82, 0.84, 0.84, 0.84, 0.82, 0.8, 0.8, 0.8, 0.78, 0.8, 0.78, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.78, 0.78, 0.76, 0.76, 0.78, 0.76, 0.74, 0.74, 0.74, 0.74, 0.72, 0.74, 0.74, 0.74, 0.76, 0.76, 0.74, 0.74, 0.72, 0.72]
```

**Check tokens in each window**

```python
print(ldvals.mattrwins[0]) #first window
print(ldvals.mattrwins[1]) #second window
```

```
['there_PRON', 'be_VERB', 'a_DET', 'saying_NOUN', 'in_ADP', 'my_PRON', 'language_NOUN', 'that_PRON', 'go_VERB', 'like_ADP', 'if_SCONJ', 'only_ADV', 'the_DET', 'young_ADJ', 'could_AUX', 'know_VERB', 'and_CCONJ', 'the_DET', 'old_ADJ', 'could_AUX', 'do_VERB', 'this_PRON', 'explain_VERB', 'an_DET', 'important_ADJ', 'lesson_NOUN', 'but_CCONJ', 'one_PRON', 'have_VERB', 'to_PART', 'attain_VERB', 'a_DET', 'certain_ADJ', 'degree_NOUN', 'of_ADP', 'wisdom_NOUN', 'to_PART', 'understand_VERB', 'it_PRON', 'in_ADP', 'my_PRON', 'opinion_NOUN', 'be_AUX', 'young_ADJ', 'be_AUX', 'more_ADV', 'enjoyable_ADJ', 'be_AUX', 'old_ADJ', 'may_AUX']

['be_VERB', 'a_DET', 'saying_NOUN', 'in_ADP', 'my_PRON', 'language_NOUN', 'that_PRON', 'go_VERB', 'like_ADP', 'if_SCONJ', 'only_ADV', 'the_DET', 'young_ADJ', 'could_AUX', 'know_VERB', 'and_CCONJ', 'the_DET', 'old_ADJ', 'could_AUX', 'do_VERB', 'this_PRON', 'explain_VERB', 'an_DET', 'important_ADJ', 'lesson_NOUN', 'but_CCONJ', 'one_PRON', 'have_VERB', 'to_PART', 'attain_VERB', 'a_DET', 'certain_ADJ', 'degree_NOUN', 'of_ADP', 'wisdom_NOUN', 'to_PART', 'understand_VERB', 'it_PRON', 'in_ADP', 'my_PRON', 'opinion_NOUN', 'be_AUX', 'young_ADJ', 'be_AUX', 'more_ADV', 'enjoyable_ADJ', 'be_AUX', 'old_ADJ', 'may_AUX', 'make_VERB']
```

**Plot mattr**
```python
print(ldvals.mattrplot)
```
<img width="300" alt="image" src="https://user-images.githubusercontent.com/84297888/154139419-60edf94d-2919-477b-b46b-fe4e1321c80b.png">


#### â–¶ï¸ŽLEARN MORE

* MATTR has been demonstrated as one of the measures that meaningfully contributes to the human judgment of LD (Kyle et al., 2021)<sup>[17](https://lcr-ads-lab.github.io/TAALED/references/1.%20Studies.html#kyle-k-crossley-s-a--jarvis-s-2021)</sup>.


## MTLD<sup>â˜…â˜…â˜…</sup>
The Measure of Textual Lexical Diversity (MTLD; McCarthy, 2005; McCarthy & Jarvis, 2010)<sup>[19][14]</sup> essentially measures the average number of tokens it takes to reach a specified TTR value (i.e., .720). There are variants of MTLD implementation.

**Report mtld value**
```python
print(ldvals.mtld)
```
```
72.41499999999999
```

**Check mtldav? value**
```python
print(ldvals.mtldav) #what's the difference between mtld and mtldav?
```
**Check mtldo value**
```python
print(ldvals.mtldo) #what's the difference between mtldo and mtld?
```
**Check mtldvals value**
```python
print(ldvals.mtldvals)
```
**Check mtldlists? value**
```python
print(ldvals.mtldlists) #should I include this?
```

**Plot mtld**
```python
print(ldvals.mtldplot)
```
<img width="300" alt="image" src="https://user-images.githubusercontent.com/84297888/154135920-0cba8556-b9fe-4ca7-b6ec-04e675f81af1.png">


#### â–¶ï¸ŽLEARN MORE

##### MTLD-Original<sup>â˜…â˜…â˜…</sup>

* In the McCarthy and Jarvis's (2010)<sup>[14]</sup> paper, each of the text segments (i.e., the number of tokens) maintaining the TTR values is called a "factor". In MTLD-Original (McCarthy & Jarvis, 2010)<sup>[14]</sup>, factors are computed uni-directionally from the beginning to the end of the document, using non-overlapping text segments (i.e., subsequent cycles of factor calculation starts at the next token in the text). The remaining tokens that do not make up a full factor is called a partial factor, and this is used to adjust the final MTLD score.

##### MTLD-MA-Bi<sup>â˜…â˜…â˜†</sup>

* Moving-average bidirectional MTLD (MTLD-MA-Bi; McCarthy & Jarvis, 2010)<sup>[14]</sup> is a revised MTLD procedure that takes a moving-average approach to compute factors. Moving-average calculation means that MTLD-MA-Bi uses the n-th token as the starting token for an n-th factor. Accordingly, the number of words required to create a factor is calculated for each progressive word in the text until a factor cannot be completed. Bidirectional means that the same procedure is repeated in backward, from the last token in the text. The final value is calculated as the average factor lengths out of all the factors.

##### MTLD-MA-Wrap<sup>â˜…â˜…â˜…</sup>

* Moving-average wrapped MTLD (MTLD-MA-Wrap; McCarthy & Jarvis, 2010)<sup>[14]</sup> is another revised method of calculating MTLD. Like MTLD-MA-Bi, it takes a moving-average approach to create factors. However, instead of working through the text in both directions, MTLD-MA-Wrap avoids partial factors by looping back to the text's beginning.

* Koizumi (2012)<sup>[20]</sup> evaluated MTLD (including simple TTR, Root TTR, and vocd-D) using spoken English samples from 20 Japanese adolescents. Each text was clipped to the first 200 words and then subdivided into 25 segments ranging in length from 50 to 200 tokens by parallel sampling. Results indicated that MTLD values stabilized at roughly 100 tokens, and none of the other indices produced stable values within the text length ranges included in the study.

* Koizumi and Inâ€™nami (2012)<sup>[6]</sup> used similar methods and obtained the same pattern of results. Among simple TTR, Root TTR, Maas, vocd-D, HD-D, and MTLD, MTLD was the only index that produced stable values, with stabilization occurring at around 100 tokens.

* Following the parallel sampling approach in Koizumi et al. (Koizumi, 2012; Koizumi & In'nami, 2012)<sup>[20][6]</sup>, Zenker and Kyle (2021)<sup>[1]</sup> evaluated MTLD using 4,542 argumentative essays by English learners from Asian regions. The result indicated that MTLD-Original and MTLD-MA-Wrap were robust to even shorter texts such as 50 words.

* McCarthy and Jarvis (2010)<sup>[14]</sup> observed a very low correlation between MTLD-Original and the text length (using a corpus comprising texts from a range of registers such as editorials, official documents, academic prose, fiction, etc.). They also found that MTLD had a low correlation with the simple TTR, which can be a desirable property of a lexical diversity index since the simple TTR strongly correlates with text length.

## HD-D<sup>â˜…â˜…â˜†</sup>

For each word type in a text, HD-D uses the hypergeometric distribution to calculate the probability of encountering one of its tokens in a random sample of 42 tokens, and these probabilities are then added together to produce the final value for the text.

**Report hdd value**
```python
print(ldvals.hdd)
```
```
0.8494048984068776
```
#### â–¶ï¸ŽLEARN MORE

* The hypergeometric distribution diversity index (HDD) is a more reliable calculation of *vocd-D* (Malvern & Richards, 1997)<sup>[18]</sup>. It relies on the probability that a word in a text would be included in a random sample from the text (McCarthy & Jarvis, 2007)<sup>[12]</sup>. For each word type in a text, HD-D uses the hypergeometric distribution to calculate the probability of encountering one of its tokens in a random sample of 42 tokens. These probabilities are then added together to produce the final HD-D value for the text. We convert this to the same scale as TTR for ease of interpretation.

* Observing that vocd-D was rapidly becoming the preferred LD measures in the field, McCarthy and Jarvis (2007)<sup>[12]</sup> conducted an assessment of its sensitivity to text length using a corpus of 23 genres taken from various corpora. As a result, they concluded that vocd-D scores were not as independent of text length. In contrast, they found a small relationship (r=.282) between text length and HD-D for longer texts (up to 2000 words).

* Koizumi and In'nami (2012)<sup>[6]</sup>, using a relatively small sample (n=38) of spoken L2 texts, found that HD-D was less affected by text length than other commonly used indices, while still reporting meaningful and significant effects.

* Zenker and Kyle (2021)<sup>[1]</sup>, using a large corpus (n = 4,542) of written L2 argumentative texts found a negligible relationship (r = 0.064) between HD-D and text length.


## Maas<sup>â˜…â˜…â˜†</sup>
<img src="https://latex.codecogs.com/svg.latex?\fn_cm&space;Maas&space;=&space;\frac{log(nTokens)-log(nTypes)}{log(nTokens)^{2}}" title="Maas = \frac{log(nTokens)-log(nTypes)}{log(nTokens)^{2}}" />

Maas' index is a more complex transformation of TTR that attempts to fit the value to a logarithmic curve. It is often referred to simply as *Maas* (Maas, 1972)<sup>[13]</sup>.

**Report maas value**
```python
print(ldvals.maas)
```
```
0.04252883206010792
```

#### â–¶ï¸ŽLEARN MORE

* Unlike most LD indices, lower Maas values are associated with higher LD.

* Maas can be categorized into a log-transformed approach to the measure of LD.

* McCarthy and Jarvis (2010)<sup>[14]</sup> showed that Maas index was one of the main predictors of their register prediction model (along with MTLD and vocd-D).

* Zenker & Kyle (2021) ... This needs to be added!

## Summary: Guideline for index selection

| Recommendation<sup>[1]</sup>               | Index (Minimum Text Length (tokens))               |
| --------------------------- | --------------------------------------------------- |
| Use with confidence (â˜…â˜…â˜…)  |[MATTR](#mattr) (50), [MTLD-Original](#mtld)(50)   |
| Use with caution (â˜…â˜…â˜†)      |[HD-D](#hd-d) (50),  [Maas](#maas) (100)              |
| [Avoid](https://lcr-ads-lab.github.io/TAALED/ld_indices/2.%20Classic_LD_indices.html#calculate-classic-but-flawed-ld-indices)  (â˜…â˜†â˜†)|MSTTR,  Log TTR,  Root TTR,  Simple TTR      
